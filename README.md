# ğŸ§ LB Playground â€” Data Engineering Playground (`lbplayground`)

This project implements a **production-inspired data pipeline** for the [ListenBrainz](https://listenbrainz.org/) dataset.
It combines **modern orchestration (Airflow)**, **SQL-first transformations (DuckDB)**, and **lightweight BI (Streamlit)** into a reproducible **Lakehouse-style architecture**.

---

## âœ… Action Plan (Progress Tracker)

* âœ… Containerization skeleton (Airflow, DuckDB, Streamlit)
* âœ… Data orchestration with **Airflow DAGs**
* âœ… **Raw â†’ Staging â†’ Curated â†’ Marts â†’ Reporting** layered architecture
* âœ… Idempotent SQL transformations (safe re-runs)
* âœ… **Slowly Changing Dimension Type 2 (SCD2)** for user names
* âœ… Daily **snapshotting** of active users
* âœ… **Streamlit dashboards** for reporting
* âœ… **Data Quality checks** (custom SQL DAG with 7 checks)
* ğŸš§ Data Governance Checks *(planned)*
* ğŸš§ CI/CD hardening (GitHub Actions full setup) *(planned)*

---

## ğŸ“‚ Repository Structure

```text
lbplayground/
â”œâ”€â”€ airflow/               
â”‚   â”œâ”€â”€ dags/              # Airflow DAGs
â”‚   â”œâ”€â”€ logs/              # Task logs
â”‚   â”œâ”€â”€ plugins/           
â”‚   â””â”€â”€ sql/               # SQL transformations
â”‚       â”œâ”€â”€ raw/           # ingestion (JSON â†’ DuckDB)
â”‚       â”œâ”€â”€ staging/       # flattening
â”‚       â”œâ”€â”€ production/    # curated, marts, dq, dc
â”‚       â”‚   â”œâ”€â”€ curated/   # fact/dim tables
â”‚       â”‚   â”œâ”€â”€ dc/        # data conformance checks
â”‚       â”‚   â”œâ”€â”€ dq/        # data quality checks
â”‚       â”‚   â””â”€â”€ marts/     # aggregates
â”‚       â”œâ”€â”€ reporting/     # reporting queries/views
â”‚       â””â”€â”€ v1/            # (initial drafts / legacy)
â”œâ”€â”€ data/                  # source ListenBrainz JSONL
â”œâ”€â”€ docker/                # Dockerfiles
â”œâ”€â”€ metabase_plugins/      # (optional BI connectors)
â”œâ”€â”€ reports/               # static screenshots for README
â”œâ”€â”€ streamlit_app/         
â”‚   â””â”€â”€ app.py             # BI dashboard
â”œâ”€â”€ wiki/                  # documentation
â”‚   â”œâ”€â”€ images/            
â”‚   â”œâ”€â”€ airflow.md         
â”‚   â”œâ”€â”€ data_dictionary.md 
â”‚   â”œâ”€â”€ data_model.md      
â”‚   â”œâ”€â”€ infra.md           
â”‚   â””â”€â”€ play_duckdb.md     
â”œâ”€â”€ .env                   # env vars
â”œâ”€â”€ .gitignore
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ Tech Stack

* **DuckDB** â€“ embedded OLAP database
* **Apache Airflow** â€“ DAG orchestration
* **Streamlit** â€“ BI/dashboarding
* **Python + SQL** â€“ ETL + transformations
* **Postgres** â€“ Airflow metadata DB
* **Docker Compose** â€“ reproducible infra
* *(Optional)* **Great Expectations** â€“ data quality checks
* **GitHub Actions** â€“ CI/CD

---

## ğŸ—ï¸ Architecture

Please find the below archtiecutre made for this, usually there is an exacly draw file attached in the repo.

  ![Architecture](wiki/images/smallarchitecture.png)

We follow a **layered Lakehouse approach** with **hybrid dimensional modeling**:

* **Raw Layer** â€“ `raw.listens_jsonl` (JSON ingestion)
* **Staging Layer** â€“ `staging.listens_flat` (flattened listens)
* **Curated Layer** â€“ `dim_user` (SCD2), `fact_listen`, `snapshots_user_activity`
* **Marts Layer** â€“ `mart_user_activity`, `mart_track_performance`
* **Reporting Layer** â€“ SQL views answering reporting questions
* **Monitoring Layer** â€“ 7 SQL checks (row counts, nulls, duplicates, future dates, referential integrity, SCD2 validity, etc.) â†’ results stored in `monitoring.data_quality_results`

ğŸ“– See: [Data Model](wiki/data_model.md) | [Data Dictionary](wiki/data_dictionary.md)

## Data Flow Architecture

        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Raw Data    â”‚   (JSONL from ListenBrainz)
        â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚  Ingest (Airflow: dag_raw_ingest)
                â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚    Raw Layer  â”‚   (raw.listens_jsonl)
        â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚  Flatten (Airflow: staging)
                â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Staging Layer â”‚   (staging.listens_flat)
        â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚  Curated Transformations
                â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Curated Layer (Production) â”‚
        â”‚ - dim_user (SCD2)            â”‚
        â”‚ - fact_listen                â”‚
        â”‚ - snapshots_user_activity    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚  Aggregations
                    â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Marts Layer         â”‚
        â”‚ - mart_user_activity  â”‚
        â”‚ - mart_track_perf.    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚  Reporting Views
                    â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Reporting Layer        â”‚
        â”‚ - Q1..Q5 reports       â”‚
        â”‚   (SQL views)          â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Streamlit BI Dashboard     â”‚
        â”‚ - Q1..Q5 tabs              â”‚
        â”‚ - Monitoring tab (DQ)      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Monitoring Layer        â”‚   (Airflow: dag_data_quality)
        â”‚ - Row count checks      â”‚
        â”‚ - Null checks           â”‚
        â”‚ - Duplicate listens     â”‚
        â”‚ - Future dates          â”‚
        â”‚ - Referential integrity â”‚
        â”‚ - SCD2 overlap check    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


## ğŸ“¸ Visuals

### Airflow UI

![Airflow UI](wiki/images/airflowui.png)

### Running Containers

![Containers](wiki/images/containers.png)

### DAGs

* **Marts & Reporting**
  ![DAG Marts Reporting](wiki/images/dag_marts_reporting.png)
* **Staging & Curated**
  ![DAG Staging Curated](wiki/images/dag_staging_curated.png)

### Data Monitoring

Data monitoring pipeline with **7 different SQL checks** (row count, nulls, duplicates, future dates, overlaps, etc.).
![Data Monitoring Table](reports/A6_Monitoring_dq_checks.png)

---

## ğŸ“Š Reporting (Exploratory Deliverables)

The following reporting views directly answer the assignment questions.
Each is defined in **`airflow/sql/reporting/`** and visualized via **Streamlit** (with screenshots in `reports/`).

### **Q1. Who are the top 10 users by number of songs listened to?**

* **SQL:** [`report_top10_users.sql`](airflow/sql/reporting/report_top10_users.sql)
* **Screenshot:** ![Top 10 Users](reports/A1_Whoarethetop10users.png)

### **Q2. How many users listened on 1st March 2019?**

* **SQL:** [`report_users_on_2019_03_01.sql`](airflow/sql/reporting/report_users_on_2019_03_01.sql)
* **Screenshot:** ![Users on 2019-03-01](reports/A2_Howmanyuserslistenedon1stMarch2019.png)

### **Q3. For every user, what was the first song the user listened to?**

* **SQL:** [`report_first_song_per_user.sql`](airflow/sql/reporting/report_first_song_per_user.sql)
* **Screenshot:** ![First Song per User](reports/A3_foreveryuser_first_song_they_listen_to.png)

### **Q4. For each user, what were the top 3 days on which they had the most listens?**

* **SQL:** [`report_top3_days_per_user.sql`](airflow/sql/reporting/report_top3_days_per_user.sql)
* **Screenshot:** ![Top 3 Days](reports/A4_top_3_listening_days_per_user.png)

### **Q5. (Optional) Development of active users in a 7-day rolling window**

* **SQL:** [`report_daily_active_users.sql`](airflow/sql/reporting/report_daily_active_users.sql)
* **Screenshot:** ![Daily Active Users](reports/A5_daily_active_users_7_window.png)


### **DATA Quality SQL checks (later move to great expectations)**

* **SQL:** [`report_daily_active_users.sql`](airflow/sql/production/dq)
* **Screenshot:** ![Daily Active Users](reports/A6_Monitoring_dq_checks.png)

---

## ğŸš€ Setup & Deployment

### 1. Clone repo

```bash
git clone https://github.com/shafaypro/lbplayground.git
cd lbplayground
```

### 2. Environment variables

Create `.env`:

NOTE: the original file i renamed to `ds.jsonl` instead of `data.txt` (this should be inside the data folder as speified(in the same level of readme.md)) !!

```env
DUCKDB_PATH=/opt/duckdb/warehouse/lb.duckdb
DATA_GLOB=/opt/data/*.jsonl
SOURCE_FILTER=spotify
AIRFLOW__CORE__LOAD_EXAMPLES=False
```

### 3. Start services

```bash
docker compose up --build
```

Services:

* **Airflow** â†’ [http://localhost:8080](http://localhost:8080) (admin/admin)
* **Streamlit** â†’ [http://localhost:8501](http://localhost:8501)

---

## â–¶ï¸ Running Workflows

1. **Raw ingestion** â€“ `dag_raw_ingest` â†’ creates `raw.listens_jsonl`
2. **Staging & Curated** â€“ `dag_staging_curated` â†’ builds `dim_user`, `fact_listen`, `snapshots_user_activity`
3. **Marts & Reporting** â€“ `dag_marts_reporting` â†’ builds marts + reporting views
4. **Monitoring** â€“ `dag_data_quality` â†’ executes 7 DQ SQL checks and stores results in `monitoring.data_quality_results`

---

## ğŸ“Š Streamlit BI

* Streamlit queries **`lb_ro.duckdb`** (read-only copy of DuckDB).
* Dashboard: [http://localhost:8501](http://localhost:8501)
* Tabs:

  * **Q1â€“Q5 reporting views**
  * **Production Setup** (orchestration & infra notes)
  * **All Reports + Monitoring** (consolidated answers + DQ status)

---

## ğŸ¢ Production Setup Notes

While this repo runs **locally with DuckDB + Airflow + Streamlit**, the architecture is designed to **scale in production**:

* **Database** â†’ Swap **DuckDB** for **Snowflake / BigQuery / Redshift** depending on infra needs
* **Orchestration** â†’ Run **Airflow on Kubernetes** (Celery/K8sExecutor)
* **BI Layer** â†’ Replace Streamlit with **Metabase, Superset, Tableau, or PowerBI**
* **Data Quality** â†’ Extend SQL checks with **Great Expectations** or **dbt tests**
* **CI/CD** â†’ Use **GitHub Actions** for DAG + SQL linting, automated tests, deployments
* **Storage** â†’ Replace local JSON with **S3/GCS/Azure Blob**, add CDC via **Kafka/Debezium**

---

## ğŸ“š Further Documentation

* [Airflow Guide](wiki/airflow.md)
* [Infrastructure Setup](wiki/infra.md)
* [Data Model](wiki/data_model.md)
* [Data Dictionary](wiki/data_dictionary.md)
* [DuckDB Usage](wiki/play_duckdb.md)

---

## âš¡ This playground demonstrates how **raw event data** evolves into **analytics-ready insights** through **Airflow + DuckDB + Streamlit** â€” end-to-end, reproducible, and production-inspired. (just for learning purposes)
